# AnimateDiff Community Workflows

Community-contributed AnimateDiff workflows for advanced video generation and video-to-video transformation.

## ğŸ¬ Available Workflows

### UltimateLCM Vid2Vid Workflow
**File:** `ultimatelcm_vid2vid_workflow.json`  
**Source:** OpenArt.ai (by jboogx/Gabriel JimÃ©nez)  
**Type:** Video-to-Video Generation  

**Description:**
Advanced AnimateDiff workflow combining UltimateLCM (Latent Consistency Model) with Vid2Vid capabilities for rapid, high-quality video transformation and generation.

**Key Features:**
- âš¡ Ultra-fast generation with LCM acceleration
- ğŸ¯ Video-to-video transformation
- ğŸ¨ Style transfer capabilities
- ğŸ”„ Motion consistency preservation
- ğŸ“± Optimized for various aspect ratios

**Requirements:**
- **GPU Memory:** 8GB+ VRAM recommended
- **Models:** AnimateDiff, LCM LoRA, SD 1.5 base model
- **Custom Nodes:** AnimateDiff Evolved, LCM nodes

**Usage:**
1. Load your source video in the video input node
2. Configure LCM settings (typically 4-8 steps)
3. Adjust motion strength and style parameters
4. Set output video dimensions and frame count
5. Run workflow for rapid video generation

**Tips:**
- Lower step counts (4-6) work best with LCM
- Use CFG scale between 1.0-2.0 for LCM
- Experiment with motion scale for different effects
- Works well with short clips (2-4 seconds)

**Community Rating:** â­â­â­â­â­ (5.0/5)  
**Downloads:** 66.1K+  
**Last Updated:** 2024

## ğŸ“ Workflow Structure

```
ultimatelcm_vid2vid_workflow.json
â”œâ”€â”€ Video Input Nodes
â”œâ”€â”€ AnimateDiff Pipeline
â”œâ”€â”€ LCM Acceleration
â”œâ”€â”€ Style Control
â””â”€â”€ Video Output
```

## ğŸ”— Related Resources

- [AnimateDiff Official Repository](https://github.com/guoyww/AnimateDiff)
- [LCM Documentation](https://huggingface.co/latent-consistency)
- [Video Generation Best Practices](../../video-processing/README.md)

## ğŸ¤ Contributing

Found improvements or variations? Please contribute back to the community!

---

*Curated from OpenArt.ai community workflows*
